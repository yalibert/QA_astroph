{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6457773-36bf-4b97-8128-858cfa355a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /storage/homefs/alibert/.local/lib/python3.11/site-packages (4.44.2)\n",
      "Requirement already satisfied: datasets in /storage/homefs/alibert/.local/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /storage/homefs/alibert/.local/lib/python3.11/site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /storage/homefs/alibert/.local/lib/python3.11/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /storage/homefs/alibert/.local/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /storage/homefs/alibert/.local/lib/python3.11/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /storage/homefs/alibert/.local/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /storage/homefs/alibert/.local/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /storage/homefs/alibert/.local/lib/python3.11/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /storage/homefs/alibert/.local/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /storage/homefs/alibert/.local/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /storage/homefs/alibert/.local/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /storage/homefs/alibert/.local/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /storage/homefs/alibert/.local/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /storage/homefs/alibert/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83adf37d-e508-4a17-a00a-487488c61b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /software.9/software/Anaconda3/2024.02-1/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /storage/homefs/alibert/.local/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9d0a69-b034-43ee-9da4-37f6f2514ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6dc435d-17b6-48cd-b9f1-f0b3c0a61948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/storage/homefs/alibert/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: How many exoplanets did the Kepler spacecraft identify?\n",
      "Answer 1: 2,600\n",
      "Question 2: How many exoplanets did the Kepler spacecraft identify during its mission?\n",
      "Answer 2: over 2,600\n",
      "Question 3: How many exoplanets did the Kepler spacecraft find?\n",
      "Answer 3: 2,600\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the pipeline for text-to-text generation\n",
    "# Specify the model and tokenizer explicitly\n",
    "question_generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"valhalla/t5-base-qa-qg-hl\",  # A model fine-tuned for question generation\n",
    "    tokenizer=\"valhalla/t5-base-qa-qg-hl\",\n",
    "    device=device  # Forces CPU usage\n",
    ")\n",
    "\n",
    "question_answerer = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"deepset/roberta-base-squad2\",  # A model fine-tuned for question answering\n",
    "    tokenizer=\"deepset/roberta-base-squad2\",\n",
    "    device=device  # Forces CPU usage\n",
    ")\n",
    "\n",
    "# Define the input context\n",
    "context = \"The Kepler spacecraft identified over 2,600 exoplanets during its mission.\"\n",
    "\n",
    "# Format the input text as required by the model\n",
    "formatted_input = f\"generate question: {context}\"\n",
    "\n",
    "# Generate text with beam search\n",
    "results = question_generator(\n",
    "    formatted_input,\n",
    "    max_length=64,\n",
    "    num_return_sequences=3,  # Number of outputs to generate\n",
    "    num_beams=3  # Enable beam search with the same number as return sequences\n",
    ")\n",
    "\n",
    "# Print the generated questions and retrieve answers\n",
    "for idx, result in enumerate(results, 1):\n",
    "    question = result['generated_text']\n",
    "    print(f\"Question {idx}: {question}\")\n",
    "    \n",
    "    # Use the question-answering pipeline to retrieve the answer\n",
    "    answer = question_answerer(question=question, context=context)\n",
    "    print(f\"Answer {idx}: {answer['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97585b89-2576-4bb0-b42f-e9210cbbb82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_pairs_from_abstract(chunks,abstract):\n",
    "    qa_pairs = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            formatted_input = f\"generate question: {chunk}\"\n",
    "\n",
    "            # Generate text with beam search\n",
    "            results = question_generator(\n",
    "                formatted_input,\n",
    "                max_length=32,\n",
    "                num_return_sequences=3,  # Number of outputs to generate\n",
    "                num_beams=5  # Enable beam search with the same number as return sequences\n",
    "            )\n",
    "            \n",
    "            for idx, result in enumerate(results, 1):\n",
    "                question = result['generated_text']    \n",
    "            # Use the question-answering pipeline to retrieve the answer\n",
    "                answer = question_answerer(question=question, context=chunk)\n",
    " \n",
    "                qa_pairs.append({\n",
    "                    \"context\": chunk,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer['answer'],\n",
    "                    \"start_positions\" : answer['start'],\n",
    "                    \"end_positions\" : answer['end'],\n",
    "                    \"large context\": abstract\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating QA: {e}\")\n",
    "    return qa_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4759f83-962b-4053-8e54-066a66c05e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#file = '/storage/homefs/alibert/CAS_NLP/Final project/samples_abstracts_large.csv\n",
    "#file available here:\n",
    "# https://www.dropbox.com/scl/fi/jphh8vfh7gfsay0xou5im/samples_abstracts_large.csv?rlkey=6yf2u5wc6ulzuagqrbho3bvzk&dl=0\n",
    "#\n",
    "df = pd.read_csv(file) \n",
    "abstracts = df['P0'].drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ecf34d-f17b-4e3c-9b19-72e34691de2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0:\n",
      "The Kepler spacecraft was launched in 2009. Its primary mission was to discover Earth-sized planets in the habitable zones of other stars. It observed over 150,000 stars and identified thousands of exoplanet candidates.\n",
      "\n",
      "Chunk 1:\n",
      "The mission was initially planned for 3.5 years but extended due to its success. Kepler's observations revolutionized our understanding of planetary systems. In 2013, it suffered a mechanical failure, ending its primary mission.\n",
      "\n",
      "Chunk 2:\n",
      "However, the spacecraft continued its work under the K2 mission. The K2 mission focused on different regions of the sky. Kepler ultimately discovered over 2,600 confirmed exoplanets.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /storage/homefs/alibert/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def split_text_into_chunks(text, num_chunks=4):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_sentences = len(sentences)\n",
    "    \n",
    "    # Calculate the size of each chunk\n",
    "    chunk_size = math.ceil(total_sentences / num_chunks)\n",
    "    \n",
    "    # Split sentences into chunks\n",
    "    chunks = [sentences[i:i + chunk_size] for i in range(0, total_sentences, chunk_size)]\n",
    "    \n",
    "    # Ensure there are exactly `num_chunks` chunks\n",
    "    if len(chunks) > num_chunks:\n",
    "        chunks = chunks[:num_chunks]\n",
    "    \n",
    "    # Join sentences back into text for each chunk\n",
    "    chunk_texts = [' '.join(chunk) for chunk in chunks]\n",
    "    \n",
    "    return chunk_texts\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"The Kepler spacecraft was launched in 2009. Its primary mission was to discover Earth-sized planets in the habitable zones of other stars. \n",
    "It observed over 150,000 stars and identified thousands of exoplanet candidates. \n",
    "The mission was initially planned for 3.5 years but extended due to its success. \n",
    "Kepler's observations revolutionized our understanding of planetary systems. \n",
    "In 2013, it suffered a mechanical failure, ending its primary mission. \n",
    "However, the spacecraft continued its work under the K2 mission. \n",
    "The K2 mission focused on different regions of the sky. \n",
    "Kepler ultimately discovered over 2,600 confirmed exoplanets.\"\"\"\n",
    "    \n",
    "chunks = split_text_into_chunks(text)\n",
    "\n",
    "# Print the three chunks\n",
    "for i, chunk in enumerate(chunks, 0):\n",
    "    print(f\"Chunk {i}:\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44738b15-99e4-46cb-88fb-2b0df7574068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b00ea3-608e-4951-983a-b982d6406515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26612 [00:00<?, ?it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  6%|▋         | 1710/26612 [20:00<4:37:20,  1.50it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 26612/26612 [5:21:21<00:00,  1.38it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 292377 Q&A pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate Q&A pairs from abstracts\n",
    "qa_pairs_fromabstracts = []\n",
    "for abstract in tqdm.tqdm(abstracts):\n",
    "    qa_pairs_fromabstracts.extend(generate_qa_pairs_from_abstract(split_text_into_chunks(abstract),abstract))\n",
    "\n",
    "print(f\"Generated {len(qa_pairs_fromabstracts)} Q&A pairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4f192a2-8aae-4fcf-bfb4-46e52bd16811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292377"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_pairs_fromabstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "200d1a0a-2093-4296-8425-5c9030190d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as 'qa_dataset_fromabstracts.json'\n"
     ]
    }
   ],
   "source": [
    "# Save the Q&A in Json format\n",
    "import json\n",
    "with open(\"qa_dataset_fromabstracts.json\", \"w\") as f:\n",
    "    json.dump(qa_pairs_fromabstracts, f, indent=2)\n",
    "\n",
    "print(\"Dataset saved as 'qa_dataset_fromabstracts.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d32a11-42a0-472d-95f1-7204581285a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
